<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->

<configuration>
  
  <property>
    <name>hive.metastore.warehouse.dir</name>
    <value>hdfs://beh001/hive/warehouse</value>
  </property>
    
  <property>
    <name>hive.exec.scratchdir</name>
    <value>/user/hive/tmp</value>
  </property>
    
  <property>
    <name>hive.querylog.location</name>
    <value>/user/hive/log</value>
  </property>
    
  <property>
    <name>javax.jdo.option.ConnectionURL</name>
    <value>jdbc:mysql://node05-cuidong.novalocal/hive?createDatabaseIfNotExist=true&amp;characterEncoding=UTF-8</value>
  </property>
  
  <property>
    <name>javax.jdo.option.ConnectionDriverName</name>
    <value>com.mysql.jdbc.Driver</value>
  </property>
    
  <property>
    <name>hive.async.log.enabled</name>
    <value>false</value>
  </property>
    
  <property>
    <name>javax.jdo.option.ConnectionUserName</name>
    <value>hadoop</value>
  </property>
    
  <property>
    <name>javax.jdo.option.ConnectionPassword</name>
    <value>hadoop</value>
  </property>
    
  <property>
    <name>hive.server2.logging.operation.log.location</name>
    <value>/opt/beh/tmp/hadoop/operation_logs</value>
  </property>
    
  <property>
    <name>hive.execution.engine</name>
    <value>spark</value>
  </property>
    
  <property>
    <name>hive.metastore.uris</name>
    <value>thrift://node01-cuidong.novalocal:9083</value>
  </property>
  
  <property>
    <name>hive.server2.thrift.port</name>
    <value>10001</value>
  </property>
    
  <property>
    <name>hive.server2.thrift.bind.host</name>
    <value>node06-cuidong.novalocal</value>
  </property>
  <property>
    <name>hive.server2.thrift.min.worker.threads</name>
    <value>5</value>
    <description>Minimum number of Thrift worker threads</description>
  </property>
  <property>
    <name>hive.server2.thrift.max.worker.threads</name>
    <value>1000</value>
    <description>Maximum number of Thrift worker threads</description>
  </property>
  <property>  
    <name>hive.server2.enable.doAs</name>
    <value>false</value>
  </property>   
   
  <property>
    <name>hive.zookeeper.quorum</name>
    <value>node01-cuidong.novalocal,node02-cuidong.novalocal,node03-cuidong.novalocal</value>
  </property>
    
  <property>
    <name>hive.zookeeper.client.port</name>
    <value>2188</value>
  </property>
    
  <property>
    <name>hive.cli.print.header</name>
    <value>true</value>
  </property>
    
  <property>
    <name>hive.cli.print.current.db</name>
    <value>true</value>
  </property>
    
  <property>
    <name>spark.master</name>
    <value>yarn-cluster</value>
  </property>
    
  <property>
    <name>spark.eventLog.enabled</name>
    <value>true</value>
  </property>
    
  <property>
    <name>spark.eventLog.dir</name>
    <value>hdfs://beh001/var/log/hadoop-spark</value>
  </property>
    
  <property>
    <name>spark.executor.memory</name>
    <value>2g</value>
  </property>
    
  <property>
    <name>spark.driver.memory</name>
    <value>1g</value>
  </property>
    
  <property>
    <name>spark.serializer</name>
    <value>org.apache.spark.serializer.KryoSerializer</value>
  </property>
    
  <property>
    <name>spark.executor.cores</name>
    <value>1</value>
  </property>
    
  <property>
    <name>spark.dynamicAllocation.enabled</name>
    <value>true</value>
  </property>
    
  <property>
    <name>spark.shuffle.service.enabled</name>
    <value>true</value>
  </property>
    
  <property>
    <name>spark.dynamicAllocation.initialExecutors</name>
    <value>1</value>
  </property>
    
  <property>
    <name>spark.executor.extraJavaOptions</name>
    <value>-XX:+UseG1GC -XX:+PrintFlagsFinal -XX:+PrintReferenceGC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintAdaptiveSizePolicy -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35</value>
  </property>
    
  <property>
    <name>spark.network.timeout</name>
    <value>300</value>
  </property>
    
  <property>
    <name>spark.driver.extraLibraryPath</name>
    <value>/opt/beh/core/hadoop/lib/native</value>
  </property>
    
  <property>
    <name>spark.executor.extraLibraryPath</name>
    <value>/opt/beh/core/hadoop/lib/native</value>
  </property>
  
  
  <property>
    <name>hive.server2.authentication</name>
    <value>KERBEROS</value>
  </property>
    
  <property>
    <name>hive.server2.authentication.kerberos.principal</name>
    <value>hs2/_HOST@BONCDSC.GREAT</value>
  </property>
    
  <property>
    <name>hive.server2.authentication.kerberos.keytab</name>
    <value>/opt/beh/metadata/key/hadoop.keytab</value>
  </property>
    
  <property>
    <name>hive.metastore.sasl.enabled</name>
    <value>true</value>
  </property>
    
  <property>
    <name>hive.metastore.kerberos.keytab.file</name>
    <value>/opt/beh/metadata/key/hadoop.keytab</value>
  </property>
    
  <property>
    <name>hive.metastore.kerberos.principal</name>
    <value>ms/_HOST@BONCDSC.GREAT</value>
  </property>
    
  <property>
    <name>hive.cluster.delegation.token.store.class</name>
    <value>org.apache.hadoop.hive.thrift.ZooKeeperTokenStore</value>
  </property>
      <property>
    <name>hive.cluster.delegation.token.store.zookeeper.connectString</name>
    <value>node01-cuidong.novalocal:2188,node02-cuidong.novalocal:2188,node03-cuidong.novalocal:2188</value>
  </property>
    
  <property>
    <name>hive.cluster.delegation.token.store.zookeeper.znode</name>
    <value>/hive/cluster/delegation</value>
  </property>
    
  <property>
    <name>hive.cluster.delegation.token.store.zookeeper.acl</name>
    <value>sasl:hs2:cdrwa,sasl:ms:cdrwa</value>
  </property>
    
  <property>
    <name>hive.server2.webui.use.ssl</name>
    <value>true</value>
  </property>
    
  <property>
    <name>hive.server2.webui.keystore.path</name>
    <value>/opt/beh/metadata/key/hadoop.keystore</value>
  </property>
    
  <property>
    <name>hive.server2.webui.keystore.password</name>
    <value>hadoop</value>
  </property>
    
  <property>
    <name>hive.server2.webui.use.spnego</name>
    <value>true</value>
  </property>
    
  <property>
    <name>hive.server2.webui.spnego.keytab</name>
    <value>/opt/beh/metadata/key/hadoop.keytab</value>
  </property>
    
  <property>
    <name>hive.server2.webui.spnego.principal</name>
    <value>HTTP/_HOST@BONCDSC.GREAT</value>
  </property>
  <property>
      <name>fs.hdfs.impl.disable.cache</name>
      <value>true</value>
  </property> 
 </configuration>
